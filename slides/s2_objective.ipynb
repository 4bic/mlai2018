{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Session 2: Objective Functions and Supervised Learning\n",
    "\n",
    "### Neil D. Lawrence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Objective Function\n",
    "\n",
    "- Last week we motivated the importance of probability.\n",
    "- This week we motivate the idea of the 'objective function'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification\n",
    "\n",
    "- In classification we take in a *feature matrix* and make predictions of *class labels* given the features. \n",
    "- Our features are $\\mathbf{x}_i$ for the $i$th data point\n",
    "- Our labels are $y_i$ which is either -1 (negative) or +1 (positive).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification\n",
    "\n",
    "- predict the class label, $y_i$, given the features associated with that data point, $\\mathbf{x}_i$, using the *prediction function*: \n",
    "\n",
    "    $$f(x_i) = \\text{sign}\\left(\\mathbf{w}^\\top \\mathbf{x}_i + b\\right)$$\n",
    "\n",
    "- Decision boundary for the classification is given by a *hyperplane*. \n",
    "- Vector $\\mathbf{w}$ is the [normal vector](http://en.wikipedia.org/wiki/Normal_(geometry)) to the hyperplane.\n",
    "- Hyperplane is described by the formula $\\mathbf{w}^\\top \\mathbf{x} = -b$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Toy Data\n",
    "\n",
    "- Need to draw a decision boundary that separates red crosses from green circles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1258e7588>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_data_per_class = 30\n",
    "np.random.seed(seed=1000001)\n",
    "x_plus = np.random.normal(loc=1.3, size=(n_data_per_class, 2))\n",
    "x_minus = np.random.normal(loc=-1.3, size=(n_data_per_class, 2))\n",
    "\n",
    "# plot data\n",
    "plt.plot(x_plus[:, 0], x_plus[:, 1], 'rx')\n",
    "plt.plot(x_minus[:, 0], x_minus[:, 1], 'go')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mathematical Drawing of Decision Boundary\n",
    "\n",
    "**Refresher**: draw a hyper plane at decision boundary.\n",
    " - *Decision boundary*: plane where a point moves from being classified as -1 to +1. \n",
    " - We have\n",
    "\n",
    "   $$\\text{sign}(\\mathbf{x}^\\top \\mathbf{w}) = \\text{sign}(w_0 + w_1x_{i,1} + w_2 x_{i, 2})$$\n",
    "\n",
    "   $x_{i, 1}$ is first feature $x_{i, 2}$ is second feature assume $x_{0,i}=1$. \n",
    " \n",
    " - Set $w_0 = b$ we have\n",
    " \n",
    "   $$\\text{sign}\\left(w_1 x_{i, 1} + w_2 x_{i, 2} + b\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Equation of Plane\n",
    "\n",
    "$$\\text{sign}\\left(w_1 x_{i, 1} + w_2 x_{i, 2} + b\\right)$$\n",
    "\n",
    "- Equation of plane is \n",
    "  \n",
    "  $$w_1 x_{i, 1} + w_2 x_{i, 2} + b = 0$$ \n",
    "  \n",
    "  or\n",
    "  \n",
    "    $$w_1 x_{i, 1} + w_2 x_{i, 2} = -b$$ \n",
    "    \n",
    "- Next we will initialise the model and draw a decision boundary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Perceptron Algorithm: Initialisation Maths\n",
    "\n",
    "- For a randomly chosen data point, $i$, set\n",
    "\n",
    "  $$\\mathbf{w} = y_i \\mathbf{x}_i$$\n",
    "\n",
    "- If predicted label of the $i$th point is \n",
    "\n",
    "  $$\\text{sign}(\\mathbf{w}^\\top\\mathbf{x}_i)$$\n",
    "\n",
    "- Setting $\\mathbf{w}$ to $y_i\\mathbf{x}_i$ implies\n",
    "\n",
    "  $$\\text{sign}(\\mathbf{w}^\\top\\mathbf{x}_i) = \\text{sign}(y_i\\mathbf{x}_i^\\top \\mathbf{x}_i) = y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Perceptron Algorithm: Initialisation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s init_perceptron mlai.py\n",
    "def init_perceptron(x_plus, x_minus, seed=1000001):\n",
    "    np.random.seed(seed=seed)\n",
    "    # flip a coin (i.e. generate a random number and check if it is greater than 0.5)\n",
    "    choose_plus = np.random.rand(1)>0.5\n",
    "    if choose_plus:\n",
    "        # generate a random point from the positives\n",
    "        index = np.random.randint(0, x_plus.shape[1])\n",
    "        x_select = x_plus[index, :]\n",
    "        w = x_plus[index, :] # set the normal vector to that point.\n",
    "        b = 1\n",
    "    else:\n",
    "        # generate a random point from the negatives\n",
    "        index = np.random.randint(0, x_minus.shape[1])\n",
    "        x_select = x_minus[index, :]\n",
    "        w = -x_minus[index, :] # set the normal vector to minus that point.\n",
    "        b = -1\n",
    "    return w, b, x_select\n",
    "\n",
    "# Routine to keep the margins of the drawing box fixed\n",
    "def margins_plot(x2, xlim, ylim, w, b):\n",
    "    if np.any(x2>3):\n",
    "        x_margin_neg = (ylim[0] + (b/w[1]))/(- w[0]/w[1])\n",
    "        x_margin_pos = (ylim[1] + (b/w[1]))/(- w[0]/w[1])\n",
    "        y_margin_neg = ylim[0]\n",
    "        y_margin_pos = ylim[1]\n",
    "    else:\n",
    "        x_margin_neg = xlim[0]\n",
    "        x_margin_pos = xlim[1]\n",
    "        y_margin_neg = (- w[0]/w[1])*xlim[0] - (b/w[1])\n",
    "        y_margin_pos = (- w[0]/w[1])*xlim[1] - (b/w[1])\n",
    "    return x_margin_neg, x_margin_pos, y_margin_neg, y_margin_pos \n",
    "# Routine for plotting\n",
    "def plot_perceptron(x1, x2, xlim, ylim, w, b):\n",
    "    x_margin_neg, x_margin_pos, y_margin_neg, y_margin_pos = margins_plot(x2, xlim, ylim, w, b)\n",
    "    x1c = (x_margin_neg + x_margin_pos)/2\n",
    "    x2c = (y_margin_neg + y_margin_pos)/2\n",
    "    x2per = (w[1]/w[0])*x1 - (w[1]/w[0])*x1c + x2c\n",
    "    plt.axes()\n",
    "    plt.plot(x_plus[:, 0], x_plus[:, 1], 'rx')\n",
    "    plt.plot(x_minus[:, 0], x_minus[:, 1], 'go')    \n",
    "    plt.xlabel(r'$x_1$', fontsize=14)\n",
    "    plt.ylabel(r'$x_2$', fontsize=14)\n",
    "    plt.plot(x1, x2, 'b')\n",
    "    plt.plot(x1, x2per, 'k')\n",
    "    plt.xlim(xlim[0], xlim[1])\n",
    "    plt.ylim(ylim[0], ylim[1])\n",
    "    plt.arrow(x1c, x2c, w[0], w[1], width=0.03, head_width=0.3, head_length=0.3, fc='k', ec='k')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Initialising the Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "npoints = 100\n",
    "xlim = np.array([-3.5, 3.5])\n",
    "ylim = xlim\n",
    "\n",
    "w = 0.5*np.random.randn(2)\n",
    "b = 0.5*np.random.randn()\n",
    "x1 = np.linspace(xlim[0], xlim[1], npoints)\n",
    "x2 = (- w[0]/w[1])*x1 - (b/w[1])\n",
    "plot_perceptron(x1, x2, xlim, ylim, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "   \n",
    "### Drawing Decision Boundary\n",
    "\n",
    "The decision boundary is where the output of the function changes from -1 to +1 (or vice versa) so it's the point at which the argument of the $\\text{sign}$ function is zero. So in other words, the decision boundary is given by the *line* defined by $x_1 w_1 + x_2 w_2 = -b$ (where we have dropped the index $i$ for convenience). In this two dimensional space the decision boundary is defined by a line. In a three dimensional space it would be defined by a *plane*  and in higher dimensional spaces it is defined by something called a *[hyperplane](http://en.wikipedia.org/wiki/Hyperplane)*. This equation is therefore often known as the *separating hyperplane* because it defines the hyperplane that separates the data. To draw it in 2-D we can choose some values to plot from $x_1$ and then find the corresponding values for $x_2$ to plot using the rearrangement of the hyperplane formula as follows\n",
    "\n",
    "$$x_2 = -\\frac{(b+x_1w_1)}{w_2}$$\n",
    "\n",
    "Of course, we can also choose to specify the values for $x_2$ and compute the values for $x_1$ given the values for $x_2$,\n",
    "\n",
    "$$x_1 = -\\frac{b + x_2w_2}{w_1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Switching Formulae\n",
    "\n",
    "It turns out that sometimes we need to use the first formula, and sometimes we need to use the second. Which formula we use depends on how the separating hyperplane leaves the plot. \n",
    "\n",
    "We want to draw the separating hyperplane in the bounds of the plot which is showing our data. To think about which equation to use, let's consider two separate situations (actually there are a few more). \n",
    "\n",
    "1. If the separating hyperplane leaves the top and bottom of the plot then we want to plot a line with values in the $y$ direction (given by $x_2$) given by the upper and lower limits of our plot. The values in the $x$ direction can then be computed from the formula for the plane. \n",
    "\n",
    "2. Conversely if the line leaves the sides of the plot then we want to plot a line with values in the $x$ direction given by the limits of the plot. Then the values in the $y$ direction can be computed from the formula. Whether the line leaves the top/bottom or the sides of the plot is dependent on the relative values of $w_1$ and $w_2$. \n",
    "\n",
    "This motivates a simple `if` statement to check which situation we're in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Code for Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s update_perceptron mlai.py\n",
    "def update_perceptron(w, b, x_plus, x_minus, learn_rate):\n",
    "    \"Update the perceptron.\"\n",
    "    # select a point at random from the data\n",
    "    choose_plus = np.random.uniform(size=1)>0.5\n",
    "    updated=False\n",
    "    if choose_plus:\n",
    "        # choose a point from the positive data\n",
    "        index = np.random.randint(x_plus.shape[0])\n",
    "        x_select = x_plus[index, :]\n",
    "        if np.dot(w, x_select)+b <= 0.:\n",
    "            # point is currently incorrectly classified\n",
    "            w += learn_rate*x_select\n",
    "            b += learn_rate\n",
    "            updated=True\n",
    "    else:\n",
    "        # choose a point from the negative data\n",
    "        index = np.random.randint(x_minus.shape[0])\n",
    "        x_select = x_minus[index, :]\n",
    "        if np.dot(w, x_select)+b > 0.:\n",
    "            # point is currently incorrectly classified\n",
    "            w -= learn_rate*x_select\n",
    "            b -= learn_rate\n",
    "            updated=True\n",
    "    return w, b, x_select, updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plots = plot.perceptron(x_plus, x_minus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('perceptron{samp:0>3}.svg', directory='./diagrams', samp=(0, plots))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Perceptron Reflection\n",
    "\n",
    " - The perceptron is an algorithm. \n",
    " - What is it doing? When will it fail?\n",
    " - We can explain the update equations and proove it converges.\n",
    " - Normally it is far easier to first define an *objective function*.\n",
    " - Also known as a:\n",
    "     - loss function\n",
    "     - error function\n",
    "     - cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Objective Functions and Regression\n",
    "\n",
    "- Classification: map feature to class label.\n",
    "- Regression: map feature to real value our *prediction function* is\n",
    "\n",
    "    $$f(x_i) = mx_i + c$$\n",
    "\n",
    "- Need an *algorithm* to fit it. \n",
    "\n",
    "- Least squares: minimize an error.\n",
    "\n",
    "$$E(m, c) = \\sum_{i=1}^n (y_i - f(x_i))^2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression\n",
    "\n",
    "- Create an artifical data set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to decide on a *true* value for $m$ and a *true* value for $c$ to use for generating the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_true = 1.4\n",
    "c_true = -3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these values to create our artificial data. The formula \n",
    "$$y_i = mx_i + c$$ is translated to code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = m_true*x+c_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Plot of Data\n",
    "We can now plot the artifical data we've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'r.', markersize=10) # plot data as red dots\n",
    "plt.xlim([-3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These points lie exactly on a straight line, that's not very realistic, let's corrupt them with a bit of Gaussian 'noise'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Noise Corrupted Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(scale=0.5, size=4) # standard deviation of the noise is 0.5\n",
    "y = m_true*x + c_true + noise\n",
    "plt.plot(x, y, 'r.', markersize=10)\n",
    "plt.xlim([-3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Contour Plot of Error Function\n",
    "\n",
    "- Visualise the error function surface, create vectors of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array of linearly separated values around m_true\n",
    "m_vals = np.linspace(m_true-3, m_true+3, 100) \n",
    "# create an array of linearly separated values ae\n",
    "c_vals = np.linspace(c_true-3, c_true+3, 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create a grid of values to evaluate the error function in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_grid, c_grid = np.meshgrid(m_vals, c_vals) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- compute the error function at each  combination of $c$ and $m$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_grid = np.zeros((100, 100))\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        E_grid[i, j] = ((y - m_grid[i, j]*x - c_grid[i, j])**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Contour Plot of Error\n",
    "\n",
    " - We can now make a contour plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s regression_contour teaching_plots.py\n",
    "def regression_contour(f, ax, m_vals, c_vals, E_grid):\n",
    "    \"Regression contour plot.\"\n",
    "    hcont = ax.contour(m_vals, c_vals, E_grid, levels=[0, 0.5, 1, 2, 4, 8, 16, 32, 64]) # this makes the contour plot \n",
    "    plt.clabel(hcont, inline=1, fontsize=15) # this labels the contours.\n",
    "\n",
    "    ax.set_xlabel('$m$', fontsize=25)\n",
    "    ax.set_ylabel('$c$', fontsize=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(5,5))\n",
    "regression_contour(f, ax, m_vals, c_vals, E_grid)\n",
    "plt.savefig('./diagrams/regression_contour.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src='./diagrams/regression_contour.svg' width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Steepest Descent\n",
    "\n",
    "- Minimize the sum of squares error function. \n",
    "- One way of doing that is gradient descent. \n",
    "- Initialize with a guess for $m$ and $c$ \n",
    "- update that guess by subtracting a portion of the gradient from the guess. \n",
    "- Like walking down a hill in the steepest direction of the hill to get to the bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Algorithm\n",
    "\n",
    "- We start with a guess for $m$ and $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_star = 0.0\n",
    "c_star = -5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Offset Gradient\n",
    "\n",
    "- Now we need to compute the gradient of the error function, firstly with respect to $c$,\n",
    "\n",
    "  $$\\frac{\\text{d}E(m, c)}{\\text{d} c} = -2\\sum_{i=1}^n (y_i - mx_i - c)$$\n",
    "\n",
    "- This is computed in python as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_grad = -2*(y-m_star*x - c_star).sum()\n",
    "print(\"Gradient with respect to c is \", c_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Deriving the Gradient\n",
    "\n",
    "To see how the gradient was derived, first note that the $c$ appears in every term in the sum. So we are just differentiating $(y_i - mx_i - c)^2$ for each term in the sum. The gradient of this term with respect to $c$ is simply the gradient of the outer quadratic, multiplied by the gradient with respect to $c$ of the part inside the quadratic. The gradient of a quadratic is two times the argument of the quadratic, and the gradient of the inside linear term is just minus one. This is true for all terms in the sum, so we are left with the sum in the gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Slope Gradient\n",
    "\n",
    "The gradient with respect tom $m$ is similar, but now the gradient of the quadratic's argument is $-x_i$ so the gradient with respect to $m$ is\n",
    "\n",
    "$$\\frac{\\text{d}E(m, c)}{\\text{d} m} = -2\\sum_{i=1}^n x_i(y_i - mx_i - c)$$\n",
    "\n",
    "which can be implemented in python (numpy) as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_grad = -2*(x*(y-m_star*x - c_star)).sum()\n",
    "print(\"Gradient with respect to m is \", m_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Update Equations\n",
    "\n",
    "\n",
    "- Now we have gradients with respect to $m$ and $c$.\n",
    "- Can update our inital guesses for $m$ and $c$ using the gradient. \n",
    "- We don't want to just subtract the gradient from $m$ and $c$, \n",
    "- We need to take a *small* step in the gradient direction. \n",
    "- Otherwise we might overshoot the minimum. \n",
    "- We want to follow the gradient to get to the minimum, the gradient changes all the time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Move in Direction of Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(7,7))\n",
    "plot.regression_contour(f, ax, m_vals, c_vals, E_grid)\n",
    "ax.plot(m_star, c_star, 'g*', markersize=20)\n",
    "ax.arrow(m_star, c_star, -m_grad*0.1, -c_grad*0.1, head_width=0.2)\n",
    "plt.savefig('./diagrams/regression_contour_step1.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src='./diagrams/regression_contour_step1.svg' width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Update Equations \n",
    "\n",
    "- The step size has already been introduced, it's again known as the learning rate and is denoted by $\\eta$. \n",
    "\n",
    "  $$c_\\text{new} \\leftarrow c_{\\text{old}} - \\eta \\frac{\\text{d}E(m, c)}{\\text{d}c}$$ \n",
    "\n",
    "- gives us an update for our estimate of $c$ (which in the code we've been calling `c_star` to represent a common way of writing a parameter estimate, $c^*$) and \n",
    "\n",
    "  $$m_\\text{new} \\leftarrow m_{\\text{old}} - \\eta \\frac{\\text{d}E(m, c)}{\\text{d}m}$$\n",
    "  \n",
    "- Giving us an update for $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Update Code\n",
    "\n",
    "- These updates can be coded as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original m was\", m_star, \"and original c was\", c_star)\n",
    "learn_rate = 0.01\n",
    "c_star = c_star - learn_rate*c_grad\n",
    "m_star = m_star - learn_rate*m_grad\n",
    "print(\"New m is\", m_star, \"and new c is\", c_star)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Iterating Updates\n",
    "\n",
    "- Fit model by descending gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "num_plots = plot.regression_contour_fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('regression_contour_fit{num:0>3}.svg', directory='./diagrams', num=(0, num_plots))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "- If $n$ is small, gradient descent is fine.\n",
    "- But sometimes (e.g. on the internet $n$ could be a billion.\n",
    "- Stochastic gradient descent is more similar to perceptron.\n",
    "- Look at gradient of one data point at a time rather than summing across *all* data points) \n",
    "- This gives a stochastic estimate of gradient.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "- The real gradient with respect to $m$ is given by \n",
    "\n",
    "  $$\\frac{\\text{d}E(m, c)}{\\text{d} m} = -2\\sum_{i=1}^n x_i(y_i - mx_i - c)$$\n",
    "\n",
    "  but it has $n$ terms in the sum. Substituting in the gradient we can see that the full update is of the form\n",
    "\n",
    "  $$m_\\text{new} \\leftarrow m_\\text{old} + 2\\eta \\left[x_1 (y_1 - m_\\text{old}x_1 - c_\\text{old}) + (x_2 (y_2 -   m_\\text{old}x_2 - c_\\text{old}) + \\dots + (x_n (y_n - m_\\text{old}x_n - c_\\text{old})\\right]$$\n",
    "\n",
    "  This could be split up into lots of individual updates\n",
    "\n",
    "$$m_1 \\leftarrow m_\\text{old} + 2\\eta \\left[x_1 (y_1 - m_\\text{old}x_1 - c_\\text{old})\\right]$$\n",
    "$$m_2 \\leftarrow m_1 + 2\\eta \\left[x_2 (y_2 - m_\\text{old}x_2 - c_\\text{old})\\right]$$\n",
    "$$m_3 \\leftarrow m_2 + 2\\eta \\left[\\dots\\right]$$\n",
    "$$m_n \\leftarrow m_{n-1} + 2\\eta \\left[x_n (y_n - m_\\text{old}x_n - c_\\text{old})\\right]$$\n",
    "\n",
    "which would lead to the same final update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Updating $c$ and $m$\n",
    "\n",
    "- In the sum we don't  $m$ and $c$ we use for computing the gradient term at each update. \n",
    "- In stochastic gradient descent we *do* change them.\n",
    "- This means it's not quite the same as steepest desceint.\n",
    "- But we  can present each data point in a random order, like we did for the perceptron.\n",
    "- This makes the algorithm suitable for large scale web use (recently this domain is know as 'Big Data') and algorithms like this are widely used by Google, Microsoft, Amazon, Twitter and Facebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "- Or more accurate, since the data is normally presented in a random order we just can write\n",
    "\n",
    "  $$m_\\text{new} = m_\\text{old} + 2\\eta\\left[x_i (y_i - m_\\text{old}x_i - c_\\text{old})\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a random point for the update\n",
    "i = np.random.randint(x.shape[0]-1)\n",
    "# update m\n",
    "m_star = m_star + 2*learn_rate*(x[i]*(y[i]-m_star*x[i] - c_star))\n",
    "# update c\n",
    "c_star = c_star + 2*learn_rate*(y[i]-m_star*x[i] - c_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SGD for Linear Regression\n",
    "\n",
    "Putting it all together in an algorithm, we can do stochastic gradient descent for our regression data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "num_plots = plot.regression_contour_sgd(x, y)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('regression_sgd_contour_fit{num:0>3}.svg', directory='./diagrams', num=(0, num_plots))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reflection on Linear Regression and Supervised Learning\n",
    "\n",
    "Think about:\n",
    "\n",
    "1. What effect does the learning rate have in the optimization? What's the effect of making it too small, what's the effect of making it too big? Do you get the same result for both stochastic and steepest gradient descent?\n",
    "\n",
    "2. The stochastic gradient descent doesn't help very much for such a small data set. It's real advantage comes when there are many, you'll see this in the lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lab Class\n",
    "\n",
    "- You will take the ideas you have learnt.\n",
    "- You will apply them in the domain of *matrix factorisation*.\n",
    "- Matrix factorization presents a different error function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reading\n",
    "- Section 1.1.3 of @Rogers:book11 for loss functions."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
