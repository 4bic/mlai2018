{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MLAI Week 9: Classification\n",
    "\n",
    "### Neil D. Lawrence\n",
    "\n",
    "### 24th November 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pods\n",
    "import mlai\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review\n",
    "\n",
    "* Last time: Looked at unsupervised learning.\n",
    "* Introduced latent variables, dimensionality reduction and clustering.\n",
    "* This time: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification\n",
    "\n",
    "* We are given a  data set containing 'inputs', $\\mathbf{X}$ and 'targets', $\\mathbf{y}$.\n",
    "* Each data point consists of an input vector $\\mathbf{x}_i$ and a class label, $y_i$.\n",
    "* For binary classification assume $y_i$ should be either $1$ (yes) or $-1$ (no).\n",
    "* Input vector can be thought of as features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification Examples\n",
    "\n",
    "* Classifiying hand written digits from binary images (automatic zip code reading)\n",
    "* Detecting faces in images (e.g. digital cameras).\n",
    "* Who a detected face belongs to (e.g. Picasa, Facebook, DeepFace, GaussianFace)\n",
    "* Classifying type of cancer given gene expression data.\n",
    "* Categorization of document types (different types of news article on the internet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reminder on the Term Bayesian\n",
    "\n",
    "* We use Bayes' rule to invert probabilities in the Bayesian approach.\n",
    "     * Bayesian is not named after Bayes' rule (v. common confusion). \n",
    "     * The term Bayesian refers to the treatment of the parameters as stochastic variables.\n",
    "     * This approach was proposed by @Laplace:memoire74 and @Bayes:doctrine63 independently.\n",
    "     * For early statisticians this was very controversial (Fisher et al).\n",
    "* The use of Bayes rule does *not* imply you are being Bayesian.\n",
    "    * It is just an application of the product rule of probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Discrete Probability\n",
    "\n",
    "* Algorithms based on *prediction* function and *objective* function.\n",
    "* For regression the *codomain* of the functions, $f(\\mathbf{X})$ was the real numbers or sometimes real vectors. \n",
    "* In classification we are given an input vector, $\\mathbf{x}$, and an associated label, $y$ which either takes the value $0$ or $1$. \n",
    "\n",
    "Our focus has been on models where the objective function is inspired by a probabilistic analysis of the problem. In particular we've argued that we answer questions about the data set by placing probability distributions over the various quantities of interest. For the case of binary classification this will normally involve introducing probability distributions for discrete variables. Such probability distributions, are in some senses easier than those for continuous variables, in particular we can represent a probability distribution over $y$, where $y$ is binary, with one value. If we specify the probability that $y=1$ with a number that is between 0 and 1, i.e. let's say that $P(y=1) = \\pi$ (here we don't mean $\\pi$ the number, we are setting $\\pi$ to be a variable) then we can specify the probability distribution through a table.\n",
    "\n",
    "| y      | 0         | 1     |\n",
    "|:------:|:---------:|:-----:|\n",
    "| $P(y)$ | $(1-\\pi)$ | $\\pi$ |\n",
    "\n",
    "Mathematically we can use a trick to implement this same table. We can use the value $y$ as a mathematical switch and write that\n",
    "$$\n",
    "P(y) = \\pi^y (1-\\pi)^{(1-y)}\n",
    "$$\n",
    "where our probability distribution is now written as a function of $y$. This probability distribution is known as the [Bernoulli distribution](http://en.wikipedia.org/wiki/Bernoulli_distribution). The Bernoulli distribution is a clever trick for mathematically switching between two probabilities if we were to write it as code it would be better described as\n",
    "```python\n",
    "def bernoulli(x, pi):\n",
    "    if y_i == 1:\n",
    "        return pi(x)\n",
    "    else:\n",
    "        return 1-pi(x)\n",
    "```\n",
    "If we insert $y=1$ then the function is equal to $\\pi$, and if we insert $y=0$ then the function is equal to $1-\\pi$. So the function recreates the table for the distribution given above.  \n",
    "\n",
    "The probability distribution is named for [Jacob Bernoulli](http://en.wikipedia.org/wiki/Jacob_Bernoulli), the swiss mathematician. In his book Ars Conjectandi he considered the distribution and the result of a number of 'trials' under the Bernoulli distribution to form the *binomial* distribution. Below is the page where he considers Pascal's triangle in forming combinations of the Bernoulli distribution to realise the binomial distribution for the outcome of positive trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Jacob Bernoulli's Bernoulli\n",
    "* Bernoulli described the Bernoulli distribution in terms of an 'urn' filled with balls.\n",
    "* There are red and black balls. There is a fixed number of balls in the urn.\n",
    "* The portion of red balls is given by $\\pi$.\n",
    "* For this reason in Bernoulli's distribution there is *epistemic* uncertainty about the distribution parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "      figure(1), clf\n",
    "      plotWidth = textWidth*0.4;\n",
    "      line([0 0 1 1], [1 0 0 1], 'linewidth', 3, 'color', blackColor)\n",
    "      set(gca, 'xlim', [0 1], 'ylim', [0 1])\n",
    "      axis equal\n",
    "      axis off\n",
    "      blackProb = 0.3;\n",
    "      ballRadius = 0.1;\n",
    "      set(gca, 'xlim', [0 1], 'ylim', [0 1])\n",
    "      t = 0:pi/24:2*pi;\n",
    "      rows = 4;\n",
    "      cols = 1/ballRadius;\n",
    "      lastRowCols = 3;\n",
    "      for row = 0:rows-1\n",
    "        if row == rows-1\n",
    "          cols = lastRowCols;\n",
    "        end\n",
    "        for col = 0:cols-1\n",
    "          ballX = col*2*ballRadius+ballRadius;\n",
    "          ballY = row*2*ballRadius + ballRadius;\n",
    "          x = ballX*ones(size(t)) + ballRadius*sin(t);\n",
    "          y = ballY*ones(size(t)) + ballRadius*cos(t);\n",
    "          if rand<blackProb\n",
    "            ballColor = blackColor;\n",
    "          else \n",
    "            ballColor = redColor;\n",
    "          end\n",
    "          handle = patch(x', y', ballColor);\n",
    "        end\n",
    "      end\n",
    "      printLatexPlot('bernoulliUrn', '../../../ml/tex/diagrams/', plotWidth);\n",
    "      %{\n",
    "    \\end{comment}\n",
    "    \\only<3>{\\centerline{\\inputdiagram{../../../ml/tex/diagrams/bernoulliUrn}}}\n",
    "  \\end{columns}\n",
    "\\end{frame}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thomas Bayes's Bernoulli\n",
    "* Bayes described the Bernoulli distribution (he didn't call it that!) in terms of a table and two balls.\n",
    "* Each ball is rolled so it comes to rest at a uniform distribution across the table.\n",
    "* The first ball comes to rest at a position that is a $\\pi$ times the width of table.\n",
    "* After placing the first ball you consider whether a second would land to the left or the right.\n",
    "* For this reason in Bayes's distribution there is considered to be *aleatoric* uncertainty about the distribution parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "      figure(1), clf\n",
    "      plotWidth = textWidth*0.4;\n",
    "      \n",
    "      line([0 0 1 1 0], [0 1 1 0 0], 'linewidth', 3, 'color', blackColor)\n",
    "      set(gca, 'xlim', [0 1], 'ylim', [0 1])\n",
    "      axis off\n",
    "      printLatexPlot('bayesBilliard0', '../../../ml/tex/diagrams/', plotWidth);\n",
    "      ballX = rand(1);\n",
    "      ballY = 0.5;\n",
    "      r = 0.1;\n",
    "      t = 0:pi/24:2*pi;\n",
    "      x = ballX*ones(size(t)) + r*sin(t);\n",
    "      y = ballY*ones(size(t)) + r*cos(t);\n",
    "      handle = patch(x', y', blackColor);\n",
    "      set(gca, 'xlim', [0 1], 'ylim', [0 1])\n",
    "      printLatexPlot('bayesBilliard1', '../../../ml/tex/diagrams/', plotWidth);\n",
    "      line([ballX ballX], [0 1], 'linestyle', ':', 'linewidth', 3, 'color', blackColor)\n",
    "      printLatexPlot('bayesBilliard2', '../../../ml/tex/diagrams/', plotWidth);\n",
    "      counter = 2;\n",
    "      for ballX = rand(1, 7)\n",
    "        ballY = 0.5;\n",
    "        counter = counter+1;\n",
    "        x = ballX*ones(size(t)) + r*sin(t);\n",
    "        y = ballY*ones(size(t)) + r*cos(t);\n",
    "        handle = patch(x', y', redColor);\n",
    "        set(gca, 'xlim', [0 1], 'ylim', [0 1])\n",
    "        printLatexPlot(['bayesBilliard' num2str(counter)], '../../../ml/tex/diagrams/', plotWidth);\n",
    "        delete(handle)\n",
    "      end\n",
    "      %{\n",
    "      \\end{comment}\n",
    "    \\column{5cm}\n",
    "    \\only<1>{\\input{../../../ml/tex/diagrams/bayesBilliard0}}\\only<2>{\\input{../../../ml/tex/diagrams/bayesBilliard1}}\\only<3>{\\input{../../../ml/tex/diagrams/bayesBilliard2}}\\only<4>{\\input{../../../ml/tex/diagrams/bayesBilliard3}}\\only<5>{\\input{../../../ml/tex/diagrams/bayesBilliard4}}\\only<6>{\\input{../../../ml/tex/diagrams/bayesBilliard5}}\\only<7>{\\input{../../../ml/tex/diagrams/bayesBilliard6}}\\only<8>{\\input{../../../ml/tex/diagrams/bayesBilliard7}}\\only<9>{\\input{../../../ml/tex/diagrams/bayesBilliard8}}\\only<10>{\\input{../../../ml/tex/diagrams/bayesBilliard9}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Likelihood in the Bernoulli\n",
    "\n",
    "* Assume data, $\\mathbf{y}$ is binary vector length $n$. \n",
    "* Assume each value was sampled independently from the Bernoulli distribution, given probability $\\pi$\n",
    "$$\n",
    "p(\\mathbf{y}|\\pi) = \\prod_{i=1}^n \\pi^{y_i} (1-\\pi)^{1-y_i}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Negative Log Likelihood\n",
    "* Minimize the negative log likelihood\n",
    "    $$E(\\pi) = -\\log p(\\mathbf{y}|\\pi) = -\\sum_{i=1}^{n} y_i \\log \\pi - \\sum_{i=1}^n (1-y_i) \\log(1-\\pi),$$\n",
    "* Take gradient with respect to the parameter $\\pi$. \n",
    "    $$\\frac{\\text{d}E(\\pi)}{\\text{d}\\pi} = -\\frac{\\sum_{i=1}^{n} y_i}{\\pi}  + \\frac{\\sum_{i=1}^n (1-y_i)}{1-\\pi},$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fixed Point\n",
    "\n",
    "* Stationary point: set derivative to zero\n",
    "    $$0 = -\\frac{\\sum_{i=1}^{n} y_i}{\\pi}  + \\frac{\\sum_{i=1}^n (1-y_i)}{1-\\pi},$$\n",
    "\n",
    "* Rearrange to form\n",
    "    $$(1-\\pi)\\sum_{i=1}^{n} y_i =   \\pi\\sum_{i=1}^n (1-y_i),$$\n",
    "\n",
    "* Giving\n",
    "    $$\\sum_{i=1}^{n} y_i =   \\pi\\left(\\sum_{i=1}^n (1-y_i) + \\sum_{i=1}^{n} y_i\\right),$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution\n",
    "\n",
    "* Recognise that $\\sum_{i=1}^n (1-y_i) + \\sum_{i=1}^{n} y_i = n$ so we have\n",
    "    $$\\pi = \\frac{\\sum_{i=1}^{n} y_i}{n}$$\n",
    "* Estimate the probability associated with the Bernoulli by setting it to the number of observed positives, divided by the total length of $y$. \n",
    "* Makes intiutive sense. \n",
    "* What's your best guess of probability for coin toss is heads when you get 47 heads from 100 tosses?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayes' Rule Reminder\n",
    "\n",
    "$$\\text{posterior} = \\frac{\\text{likelihood}\\times\\text{prior}}{\\text{marginal likelihood}}$$\n",
    "\n",
    "* Four components:\n",
    "    1. Prior distribution: represents belief about parameter values before seeing data.\n",
    "    2. Likelihood: gives relation between parameters and data.\n",
    "    3. Posterior distribution: represents updated belief about parameters after data is observed.\n",
    "    4. Marginal likelihood: represents assessment of the quality of the model. Can be compared with other models (likelihood/prior combinations). Ratios of marginal likelihoods are known as Bayes factors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes Classifiers\n",
    "\n",
    "* First lecture: placing probability distributions (or densities) over all the variables of interest.\n",
    "* In Naive Bayes this is exactly what we do.\n",
    "* Form a classification algorithm by modelling the *joint* density of our observations. \n",
    "* Need to make assumption about joint density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Assumptions about Density\n",
    "\n",
    "* Make assumptions to reduce the number of parameters we need to optimise. \n",
    "* Given label data $\\mathbf{y}$ and the inputs $\\mathbf{X}$ could specify joint density of all potential values of $\\mathbf{y}$ and $\\mathbf{X}$, $p(\\mathbf{y}, \\mathbf{X})$. \n",
    "* If $\\mathbf{X}$ and $\\mathbf{y}$ are training data.\n",
    "* If $\\mathbf{x}^*$ is a test input and $y^*$ a test location we want\n",
    "$$\n",
    "p(y^*|\\mathbf{X}, \\mathbf{y}, \\mathbf{x}^*),\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Answer from Rules of Probability\n",
    "\n",
    "* Compute this distribution using the product and sum rules. \n",
    "* To specify this density need the probability associated with all possible combinations of $\\mathbf{y}$ and $\\mathbf{X}$. There are $2^n$ possible combinations for the vector $\\mathbf{y}$ and the probability for each of these combinations must be jointly specified along with the joint density of the matrix $\\mathbf{X}$, as well as being able to *extend* the density for any chosen test location $\\mathbf{x}^*$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes Assumptions\n",
    "\n",
    "* In naive Bayes we make certain simplifying assumptions that allow us to perform all of the above in practice. \n",
    "\n",
    "1. Data Conditional Independence\n",
    "2. Feature conditional independence\n",
    "3. Marginal density for $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Conditional Independence\n",
    "\n",
    "* Given model parameters $\\boldsymbol{\\theta}$ we assume that all data points in the model are independent. \n",
    "$$\n",
    "p(y^*, \\mathbf{x}^*, \\mathbf{y}, \\mathbf{X}|\\boldsymbol{\\theta}) = p(y^*, \\mathbf{x}^*|\\boldsymbol{\\theta})\\prod_{i=1}^n p(y_i, \\mathbf{x}_i | \\boldsymbol{\\theta}).\n",
    "$$\n",
    "* This is a conditional independence assumption.\n",
    "* We made similar assumptions for regression (where $\\boldsymbol{\\theta} = \\left\\{\\mathbf{w},\\sigma^2\\right\\}$.\n",
    "* Here we assume *joint* density of $\\mathbf{y}$ and $\\mathbf{X}$ is independent across the data given the parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayes Classifier\n",
    "\n",
    "Computing posterior distribution in this case becomes easier, this is known as the 'Bayes classifier'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Conditional Independence\n",
    "\n",
    "* Particular to naive Bayes: assume *features* are also conditionally independent, given param *and* the label. \n",
    "    $$p(\\mathbf{x}_i | y_i, \\boldsymbol{\\theta}) = \\prod_{j=1}^p p(x_{i,j}|y_i, \\boldsymbol{\\theta})$$\n",
    "  where $p$ is the dimensionality of our inputs.\n",
    "* This is known as the *naive Bayes* assumption.\n",
    "* Bayes classifier + feature conditional independence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Marginal Density for $y_i$\n",
    "\n",
    "* To specify the joint distribution we also need the marginal for $p(y_i)$\n",
    "    $$p(x_{i,j},y_i| \\boldsymbol{\\theta}) = p(x_{i,j}|y_i, \\boldsymbol{\\theta})p(y_i).$$\n",
    "* Because $y_i$ is binary the *Bernoulli* density makes a suitable choice for our prior over $y_i$,\n",
    "    $$p(y_i|\\pi) = \\pi^{y_i} (1-\\pi)^{1-y_i}$$\n",
    "  where $\\pi$ now has the interpretation as being the *prior* probability that the classification should be positive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Density for Naive Bayes\n",
    "\n",
    "This allows us to write down the full joint density of the training data,\n",
    "$$\n",
    "p(\\mathbf{y}, \\mathbf{X}|\\boldsymbol{\\theta}, \\pi) = \\prod_{i=1}^n \\prod_{j=1}^p p(x_{i,j}|y_i, \\boldsymbol{\\theta})p(y_i|\\pi)\n",
    "$$\n",
    "which can now be fit by maximum likelihood. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood\n",
    "\n",
    "As normal we form our objective as the negative log likelihood,\n",
    "$$\n",
    "E(\\boldsymbol{\\theta}, \\pi) = -\\log p(\\mathbf{y}, \\mathbf{X}|\\boldsymbol{\\theta}, \\pi) = -\\sum_{i=1}^n \\sum_{j=1}^p \\log p(x_{i, j}|y_i, \\boldsymbol{\\theta}) - \\sum_{i=1}^n \\log p(y_i|\\pi),\n",
    "$$\n",
    "which we note *decomposes* into two objective functions, one which is dependent on $\\pi$ alone and one which is dependent on $\\boldsymbol{\\theta}$ alone so we have,\n",
    "$$\n",
    "E(\\pi, \\boldsymbol{\\theta}) = E(\\boldsymbol{\\theta}) + E(\\pi).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Prior\n",
    "Since the two objective functions are separately dependent on the parameters $\\pi$ and $\\boldsymbol{\\theta}$ we can minimize them independently. Firstly, minimizing the Bernoulli likelihood over the labels we have, \n",
    "$$\n",
    "E(\\pi) = - \\sum_{i=1}^n\\log p(y_i|\\pi) = -\\sum_{i=1}^n y_i \\log \\pi - \\sum_{i=1}^n (1-y_i) \\log (1-\\pi)\n",
    "$$\n",
    "which we already minimized above recovering \n",
    "$$\n",
    "\\pi = \\frac{\\sum_{i=1}^n y_i}{n}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fit Conditional \n",
    "\n",
    "We now need to minimize the objective associated with the conditional distributions for the features,\n",
    "$$\n",
    "E(\\boldsymbol{\\theta}) = -\\sum_{i=1}^n \\sum_{j=1}^p \\log p(x_{i, j} |y_i, \\boldsymbol{\\theta}),\n",
    "$$\n",
    "which necessarily implies making some assumptions about the form of the conditional distributions. The right assumption will depend on the nature of our input data. For example, if we have an input which is real valued, we could use a Gaussian density and we could allow the mean and variance of the Gaussian to be different according to whether the class was positive or negative and according to which feature we were measuring. That would give us the form,\n",
    "$$\n",
    "p(x_{i, j} | y_i,\\boldsymbol{\\theta}) = \\frac{1}{\\sqrt{2\\pi \\sigma_{y_i,j}^2}} \\exp \\left(-\\frac{(x_{i,j} - \\mu_{y_i, j})^2}{\\sigma_{y_i,j}^2}\\right),\n",
    "$$\n",
    "where $\\sigma_{1, j}^2$ is the variance of the density for the $j$th output and the class $y_i=1$ and $\\sigma_{0, j}^2$ is the variance if the class is 0. The means can vary similarly. Our parameters, $\\boldsymbol{\\theta}$ would consist of all the means and all the variances for the different dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Making Predictions\n",
    "\n",
    "Naive Bayes has given us the class conditional densities: $p(\\mathbf{x}_i | y_i, \\boldsymbol{\\theta})$. To make predictions with these densities we need to form the distribution given by\n",
    "$$\n",
    "P(y^*| \\mathbf{y}, \\mathbf{X}, \\mathbf{x}^*, \\boldsymbol{\\theta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This can be computed by using the product rule. We know that\n",
    "$$\n",
    "P(y^*| \\mathbf{y}, \\mathbf{X}, \\mathbf{x}^*, \\boldsymbol{\\theta})p(\\mathbf{y}, \\mathbf{X}, \\mathbf{x}^*|\\boldsymbol{\\theta}) = p(y*, \\mathbf{y}, \\mathbf{X}, \\mathbf{x}^*| \\boldsymbol{\\theta})\n",
    "$$\n",
    "implying that \n",
    "$$\n",
    "P(y^*| \\mathbf{y}, \\mathbf{X}, \\mathbf{x}^*, \\boldsymbol{\\theta}) = \\frac{p(y*, \\mathbf{y}, \\mathbf{X}, \\mathbf{x}^*| \\boldsymbol{\\theta})}{p(\\mathbf{y}, \\mathbf{X}, \\mathbf{x}^*|\\boldsymbol{\\theta})}\n",
    "$$\n",
    "and we've already defined $p(y*, \\mathbf{y}, \\mathbf{X}, \\mathbf{x}^*| \\boldsymbol{\\theta})$ using our conditional "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "independence assumptions above \n",
    "$$\n",
    "p(y^*, \\mathbf{y}, \\mathbf{X}, \\mathbf{x}^*| \\boldsymbol{\\theta}) = \\prod_{j=1}^p p(x^*_{j}|y^*, \\boldsymbol{\\theta})p(y^*|\\pi)\\prod_{i=1}^n \\prod_{j=1}^p p(x_{i,j}|y_i, \\boldsymbol{\\theta})p(y_i|\\pi)\n",
    "$$\n",
    "The other required density is $$p(\\mathbf{y}, \\mathbf{X}, \\mathbf{x}^*|\\boldsymbol{\\theta})$$ which can be found from $$p(y*, \\mathbf{y}, \\mathbf{X}, \\mathbf{x}^*| \\boldsymbol{\\theta})$$ using the *sum rule* of probability,\n",
    "$$\n",
    "p(\\mathbf{y}, \\mathbf{X}, \\mathbf{x}^*|\\boldsymbol{\\theta}) = \\sum_{y^*=0}^1 p(y*, \\mathbf{y}, \\mathbf{X}, \\mathbf{x}^*| \\boldsymbol{\\theta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Because of our independence assumptions that is simply equal to \n",
    "$$p(\\mathbf{y}, \\mathbf{X}, \\mathbf{x}^*| \\boldsymbol{\\theta}) = \\sum_{y^*=0}^1 \\prod_{j=1}^p p(x^*_{j}|y^*_i, \\boldsymbol{\\theta})p(y^*|\\pi)\\prod_{i=1}^n \\prod_{j=1}^p p(x_{i,j}|y_i, \\boldsymbol{\\theta})p(y_i|\\pi).\n",
    "$$\n",
    "Substituting both forms in to recover our distribution over the test label conditioned on the training data we have,\n",
    "$$\n",
    "P(y^*| \\mathbf{y}, \\mathbf{X}, \\mathbf{x}^*, \\boldsymbol{\\theta})  = \\frac{\\prod_{j=1}^p p(x^*_{j}|y^*_i, \\boldsymbol{\\theta})p(y^*|\\pi)\\prod_{i=1}^n \\prod_{j=1}^p p(x_{i,j}|y_i, \\boldsymbol{\\theta})p(y_i|\\pi)}{\\sum_{y^*=0}^1 \\prod_{j=1}^p p(x^*_{j}|y^*_i, \\boldsymbol{\\theta})p(y^*|\\pi)\\prod_{i=1}^n \\prod_{j=1}^p p(x_{i,j}|y_i, \\boldsymbol{\\theta})p(y_i|\\pi)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "and we notice that all the terms associated with the training data actually cancel, the test prediction is *conditionally independent* of the training data *given* the parameters. This is a result of our conditional independence assumptions over the data points.\n",
    "$$\n",
    "p(y^*| \\mathbf{x}^*, \\boldsymbol{\\theta}) = \\frac{\\prod_{j=1}^p p(x^*_{j}|y^*_i, \\boldsymbol{\\theta})p(y^*|\\pi)}{\\sum_{y^*=0}^1 \\prod_{j=1}^p p(x^*_{j}|y^*_i, \\boldsymbol{\\theta})p(y^*|\\pi)}\n",
    "$$\n",
    "This formula is also fairly straightforward to implement. First we implement the log probabilities for the Gaussian density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reading\n",
    "\n",
    "-   Chapter 5 of @Rogers:book11 up to pg 179 (Section 5.1, and 5.2 up to 5.2.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
